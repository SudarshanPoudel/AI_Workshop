{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa0eac6",
   "metadata": {},
   "source": [
    "# Classification and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0b3f7",
   "metadata": {},
   "source": [
    "## Step 1. Read dataset\n",
    "\n",
    "We will use an SMS dataset where each message is labeled as **spam** or **ham** (not spam).\n",
    "\n",
    "This dataset will help us understand how text data is handled in machine learning and how NLP preprocessing fits into a classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d806dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../datasets/sms.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a358043",
   "metadata": {},
   "source": [
    "## Step 2. Setup NLP Tools and Simple preprocessing\n",
    "\n",
    "Natural Language Processing often relies on external language resources such as:\n",
    "- Tokenizers\n",
    "- Stopword lists\n",
    "- Word dictionaries\n",
    "\n",
    "NLTK provides these resources, which need to be downloaded once before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary data from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdddff",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units called **tokens** (usually words).\n",
    "\n",
    "Here, we compare:\n",
    "- A simple string split\n",
    "- NLTK’s tokenizer, which handles punctuation and contractions better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Don't split contractions badly! It's important.\"\n",
    "text = text.lower()\n",
    "basic_split = text.split()\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Basic split: {basic_split}\")\n",
    "print(f\"NLTK tokens: {nltk_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c3e91",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Stop words are very common words such as *the, is, and, to*.\n",
    "\n",
    "These words usually do not add much meaning for tasks like spam detection, so we often remove them to reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Without stop words: {filtered_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb36a2",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming reduces words to their root form by applying simple rules.\n",
    "\n",
    "The goal is to treat similar words (e.g., *running* and *runs*) as the same feature, even if the resulting word is not grammatically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words_to_stem = ['running', 'runs', 'easily', 'studies', 'happiness']\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} → {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff97da9",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming, but it converts words into their **dictionary form** (lemma).\n",
    "\n",
    "Compared to stemming, lemmatization:\n",
    "- Is more accurate\n",
    "- Is more linguistically correct\n",
    "- Is usually slower\n",
    "\n",
    "We compare both approaches to understand the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word} → {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Compare stemming vs lemmatization\n",
    "print(\"\\nStemming vs Lemmatization:\")\n",
    "comparison_words = ['better', 'running', 'studies', 'geese', 'feet']\n",
    "for word in comparison_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word} → Stem: {stem}, Lemma: {lemma}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c554dd",
   "metadata": {},
   "source": [
    "## Text Cleaning Pipeline\n",
    "\n",
    "Instead of applying each preprocessing step manually every time, we combine them into a single function.\n",
    "\n",
    "This function performs:\n",
    "- Lowercasing\n",
    "- Removing special characters\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Stemming\n",
    "\n",
    "This makes preprocessing reusable and consistent across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Setup preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"      This is very &&!Good Text. Visit www.testdoc.com for more info or mail us at a@gmail.com! or 981111111 \"\n",
    "cleaned = clean_text(original)\n",
    "\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sms'] = df['sms'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ad26b",
   "metadata": {},
   "source": [
    "## Step 3. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['sms'],\n",
    "    df['class'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['class']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f6eba",
   "metadata": {},
   "source": [
    "## Step 4. Feature Extraction: Representing Text as Numbers\n",
    "\n",
    "Machine learning models cannot work directly with text.\n",
    "They only understand numbers.\n",
    "\n",
    "Feature extraction is the process of converting raw text into a numerical representation that a model can learn from.\n",
    "\n",
    "In this step, we will use a simple and widely used approach called **Bag of Words (BoW)** to represent text as numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f70bf",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "The Bag of Words approach represents text by:\n",
    "- Building a vocabulary of all unique words in the dataset\n",
    "- Counting how many times each word appears in a message\n",
    "\n",
    "Important points:\n",
    "- Word order is ignored\n",
    "- Only word frequency matters\n",
    "- Each word becomes a feature (column)\n",
    "\n",
    "In scikit-learn, this is implemented using **CountVectorizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865de84",
   "metadata": {},
   "source": [
    "### Convert Text into Numeric Features\n",
    "\n",
    "We use `CountVectorizer` to:\n",
    "- Learn the vocabulary from the training data\n",
    "- Convert each message into a vector of word counts\n",
    "\n",
    "The training and test data are transformed using the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99662f3d",
   "metadata": {},
   "source": [
    "### Inspect the Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_df = pd.DataFrame(\n",
    "    X_train_vec[:5].toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "X_train_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ea27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_df.loc[:, (X_train_vec_df != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad947f6b",
   "metadata": {},
   "source": [
    "### Possible Improvements (Not Implemented Here)\n",
    "\n",
    "In practice, models can be improved by adding more features, such as:\n",
    "- TF-IDF instead of raw word counts\n",
    "- N-grams (word pairs or triples)\n",
    "- Message length\n",
    "- Number of digits or special characters\n",
    "- Presence of URLs or phone numbers\n",
    "\n",
    "For this workshop, we keep the feature extraction simple, but students are encouraged to experiment with these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef3f61",
   "metadata": {},
   "source": [
    "## Step 5. Model Training\n",
    "\n",
    "Now that the text has been converted into numerical features, we can train a classification model.\n",
    "\n",
    "We will use **Logistic Regression**, a commonly used baseline model for text classification problems such as spam detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774585ff",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7586ae3",
   "metadata": {},
   "source": [
    "### Predict on New Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75733dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\"Free entry in 2 a weekly competition to win prizes\", \"URGENT! You have won a 1 week FREE membership\"]\n",
    "message_vec = vectorizer.transform(message)\n",
    "\n",
    "prediction = model.predict(message_vec)\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(message_vec)\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae189ba",
   "metadata": {},
   "source": [
    "### Class Distribution\n",
    "\n",
    "Before relying only on accuracy, it is important to check how balanced the dataset is.\n",
    "\n",
    "If one class appears much more frequently than the other, accuracy alone can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827076f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf79346",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score combines **precision** and **recall** into a single metric.\n",
    "\n",
    "Here, we focus on the F1 score for the **spam** class, since correctly identifying spam is usually more important than ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eff2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred, pos_label=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = [\"ham\", \"spam\"]\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual ham\", \"Actual spam\"],\n",
    "    columns=[\"Pred ham\", \"Pred spam\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
